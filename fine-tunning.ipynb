{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:23.680520Z",
     "iopub.status.busy": "2025-09-10T22:22:23.680104Z",
     "iopub.status.idle": "2025-09-10T22:22:23.685058Z",
     "shell.execute_reply": "2025-09-10T22:22:23.684235Z",
     "shell.execute_reply.started": "2025-09-10T22:22:23.680500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import Levenshtein\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:23.696582Z",
     "iopub.status.busy": "2025-09-10T22:22:23.696405Z",
     "iopub.status.idle": "2025-09-10T22:22:23.701930Z",
     "shell.execute_reply": "2025-09-10T22:22:23.701405Z",
     "shell.execute_reply.started": "2025-09-10T22:22:23.696568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1  \n",
    "print(f\"Device set to use: {'cuda:0' if device==0 else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:23.710705Z",
     "iopub.status.busy": "2025-09-10T22:22:23.710511Z",
     "iopub.status.idle": "2025-09-10T22:22:25.192104Z",
     "shell.execute_reply": "2025-09-10T22:22:25.191345Z",
     "shell.execute_reply.started": "2025-09-10T22:22:23.710691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"vennify/t5-base-grammar-correction\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:25.193456Z",
     "iopub.status.busy": "2025-09-10T22:22:25.193194Z",
     "iopub.status.idle": "2025-09-10T22:22:25.223674Z",
     "shell.execute_reply": "2025-09-10T22:22:25.223007Z",
     "shell.execute_reply.started": "2025-09-10T22:22:25.193438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ungrammatical Statement</th>\n",
       "      <th>Standard English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I goes to the store everyday.</td>\n",
       "      <td>I go to the store everyday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They was playing soccer last night.</td>\n",
       "      <td>They were playing soccer last night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She have completed her homework.</td>\n",
       "      <td>She has completed her homework.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He don't know the answer.</td>\n",
       "      <td>He doesn't know the answer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sun rise in the east.</td>\n",
       "      <td>The sun rises in the east.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Ungrammatical Statement                      Standard English\n",
       "0        I goes to the store everyday.           I go to the store everyday.\n",
       "1  They was playing soccer last night.  They were playing soccer last night.\n",
       "2     She have completed her homework.       She has completed her homework.\n",
       "3            He don't know the answer.           He doesn't know the answer.\n",
       "4            The sun rise in the east.            The sun rises in the east."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/kaggle/input/grammar-correction/Grammar Correction.csv')\n",
    "df.drop(columns={'Serial Number','Error Type'},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:25.224646Z",
     "iopub.status.busy": "2025-09-10T22:22:25.224416Z",
     "iopub.status.idle": "2025-09-10T22:22:25.230424Z",
     "shell.execute_reply": "2025-09-10T22:22:25.229740Z",
     "shell.execute_reply.started": "2025-09-10T22:22:25.224626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1045, Val: 262\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:25.232228Z",
     "iopub.status.busy": "2025-09-10T22:22:25.231976Z",
     "iopub.status.idle": "2025-09-10T22:22:25.526166Z",
     "shell.execute_reply": "2025-09-10T22:22:25.525632Z",
     "shell.execute_reply.started": "2025-09-10T22:22:25.232211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e305edd9cc3416abe8289397a8601fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176bbb48e0484ba0b088a7736f7a3aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['Ungrammatical Statement'], \n",
    "                     text_target=batch['Standard English'], \n",
    "                     padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:22:25.527134Z",
     "iopub.status.busy": "2025-09-10T22:22:25.526873Z",
     "iopub.status.idle": "2025-09-10T22:22:25.531194Z",
     "shell.execute_reply": "2025-09-10T22:22:25.530547Z",
     "shell.execute_reply.started": "2025-09-10T22:22:25.527111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:31:55.620061Z",
     "iopub.status.busy": "2025-09-10T22:31:55.619206Z",
     "iopub.status.idle": "2025-09-10T22:31:56.438834Z",
     "shell.execute_reply": "2025-09-10T22:31:56.438152Z",
     "shell.execute_reply.started": "2025-09-10T22:31:55.620031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def precision_recall_fbeta(y_true: List[List[str]], y_pred: List[List[str]], beta: float = 0.5):\n",
    "    assert len(y_true) == len(y_pred), \"Predictions and references must have the same length\"\n",
    "    total_correct, total_pred, total_true = 0, 0, 0\n",
    "    for ref, pred in zip(y_true, y_pred):\n",
    "        ref_set, pred_set = set(ref), set(pred)\n",
    "        correct = len(ref_set & pred_set)\n",
    "        total_correct += correct\n",
    "        total_pred += len(pred_set)\n",
    "        total_true += len(ref_set)\n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0\n",
    "    recall = total_correct / total_true if total_true > 0 else 0\n",
    "    if precision + recall == 0:\n",
    "        fbeta = 0\n",
    "    else:\n",
    "        fbeta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return precision, recall, fbeta\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # BLEU\n",
    "    references = [[ref] for ref in labels]\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references)[\"bleu\"]\n",
    "\n",
    "    ned_values = [\n",
    "        Levenshtein.distance(pred, ref) / max(len(ref), 1)\n",
    "        for pred, ref in zip(predictions, labels)\n",
    "    ]\n",
    "    ned_score = sum(ned_values) / len(ned_values)\n",
    "\n",
    "    # F0.5, Precision, Recall\n",
    "    y_true = [ref.split() for ref in labels]\n",
    "    y_pred = [pred.split() for pred in predictions]\n",
    "    precision, recall, f0_5 = precision_recall_fbeta(y_true, y_pred, beta=0.5)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"ned\": ned_score,\n",
    "        \"f0.5\": f0_5,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:31:56.440243Z",
     "iopub.status.busy": "2025-09-10T22:31:56.439984Z",
     "iopub.status.idle": "2025-09-10T22:31:56.458887Z",
     "shell.execute_reply": "2025-09-10T22:31:56.458020Z",
     "shell.execute_reply.started": "2025-09-10T22:31:56.440214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:31:57.831920Z",
     "iopub.status.busy": "2025-09-10T22:31:57.831173Z",
     "iopub.status.idle": "2025-09-10T22:31:57.838634Z",
     "shell.execute_reply": "2025-09-10T22:31:57.837905Z",
     "shell.execute_reply.started": "2025-09-10T22:31:57.831897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T22:32:12.030685Z",
     "iopub.status.busy": "2025-09-10T22:32:12.030431Z",
     "iopub.status.idle": "2025-09-10T22:48:35.382628Z",
     "shell.execute_reply": "2025-09-10T22:48:35.381948Z",
     "shell.execute_reply.started": "2025-09-10T22:32:12.030666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 metrics: {'bleu': 0.8031517477125393, 'ned': 0.09526589104174304, 'f0.5': 0.9073637316561846, 'precision': 0.9040469973890339, 'recall': 0.9208776595744681}\n",
      " Best model saved with F0.5=0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.0166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 metrics: {'bleu': 0.8389963609599745, 'ned': 0.06875569806186241, 'f0.5': 0.927055702917772, 'precision': 0.9264413518886679, 'recall': 0.9295212765957447}\n",
      " Best model saved with F0.5=0.9271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 metrics: {'bleu': 0.8454687243969327, 'ned': 0.06520899699093696, 'f0.5': 0.927055702917772, 'precision': 0.9264413518886679, 'recall': 0.9295212765957447}\n",
      "No improvement for 1 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.0229] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 metrics: {'bleu': 0.8545749279730742, 'ned': 0.05949677989210565, 'f0.5': 0.93367889420521, 'precision': 0.9335548172757475, 'recall': 0.9341755319148937}\n",
      " Best model saved with F0.5=0.9337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.0137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 metrics: {'bleu': 0.8595363258032627, 'ned': 0.05718402015903243, 'f0.5': 0.9371686108165429, 'precision': 0.9364238410596026, 'recall': 0.9401595744680851}\n",
      " Best model saved with F0.5=0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 metrics: {'bleu': 0.8639581227547694, 'ned': 0.05608681840992979, 'f0.5': 0.9373343932167462, 'precision': 0.9364659166115156, 'recall': 0.9408244680851063}\n",
      " Best model saved with F0.5=0.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 metrics: {'bleu': 0.8613403928172374, 'ned': 0.065139966589571, 'f0.5': 0.9346965699208444, 'precision': 0.9328505595786701, 'recall': 0.942154255319149}\n",
      "No improvement for 1 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 metrics: {'bleu': 0.8685632295249467, 'ned': 0.06286894749437304, 'f0.5': 0.9351900739176346, 'precision': 0.9334650856389987, 'recall': 0.942154255319149}\n",
      "No improvement for 2 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.000501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 metrics: {'bleu': 0.8720152978311988, 'ned': 0.06001097441747431, 'f0.5': 0.9371693121693123, 'precision': 0.9359313077939234, 'recall': 0.942154255319149}\n",
      "No improvement for 3 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.000367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 metrics: {'bleu': 0.8744012841658286, 'ned': 0.053976153074022565, 'f0.5': 0.9421443736730362, 'precision': 0.9416445623342176, 'recall': 0.9441489361702128}\n",
      " Best model saved with F0.5=0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.0104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 metrics: {'bleu': 0.8714412478544773, 'ned': 0.05253640035022266, 'f0.5': 0.94115302869288, 'precision': 0.9409030544488712, 'recall': 0.942154255319149}\n",
      "No improvement for 1 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.000486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 metrics: {'bleu': 0.8757467530825537, 'ned': 0.05006835095798728, 'f0.5': 0.9441489361702127, 'precision': 0.9441489361702128, 'recall': 0.9441489361702128}\n",
      " Best model saved with F0.5=0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 metrics: {'bleu': 0.8804829458579425, 'ned': 0.04812170820793159, 'f0.5': 0.9466277217206586, 'precision': 0.9462508294625083, 'recall': 0.9481382978723404}\n",
      " Best model saved with F0.5=0.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 metrics: {'bleu': 0.8735249641280345, 'ned': 0.05201391835490172, 'f0.5': 0.9438232161874334, 'precision': 0.9440745672436751, 'recall': 0.9428191489361702}\n",
      "No improvement for 1 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.000444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 metrics: {'bleu': 0.8724702110073912, 'ned': 0.0551999440171092, 'f0.5': 0.9414724576271187, 'precision': 0.9404761904761905, 'recall': 0.9454787234042553}\n",
      "No improvement for 2 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=0.00029] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 metrics: {'bleu': 0.8700852779546421, 'ned': 0.054260161135533974, 'f0.5': 0.9428191489361701, 'precision': 0.9428191489361702, 'recall': 0.9428191489361702}\n",
      "No improvement for 3 evaluations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 103/103 [00:52<00:00,  1.96it/s, loss=3.77e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 metrics: {'bleu': 0.874689534907623, 'ned': 0.05967109833227765, 'f0.5': 0.942297511911064, 'precision': 0.9411764705882353, 'recall': 0.9468085106382979}\n",
      "No improvement for 4 evaluations.\n",
      " Early stopping triggered after 4 evaluations without improvement.\n"
     ]
    }
   ],
   "source": [
    "patience = 4    \n",
    "counter = 0   \n",
    "best_f05 = 0\n",
    "\n",
    "for epoch in range(20):  \n",
    "    model.train()\n",
    "    loop = tqdm(train_loader)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            labels=batch['labels'].to(device)\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model.generate(\n",
    "                batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device)\n",
    "            )\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            labels = [tokenizer.decode(l, skip_special_tokens=True) for l in batch['labels']]\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    \n",
    "    metrics = compute_metrics((all_preds, all_labels))\n",
    "    print(f\"Epoch {epoch+1} metrics:\", metrics)\n",
    "\n",
    "    \n",
    "    if metrics['f0.5'] > best_f05:\n",
    "        best_f05 = metrics['f0.5']\n",
    "        counter = 0    \n",
    "        model.save_pretrained(\"./best_model\")\n",
    "        tokenizer.save_pretrained(\"./best_model\")\n",
    "        print(f\" Best model saved with F0.5={best_f05:.4f}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement for {counter} evaluations.\")\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\" Early stopping triggered after {counter} evaluations without improvement.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T23:12:23.985983Z",
     "iopub.status.busy": "2025-09-10T23:12:23.985708Z",
     "iopub.status.idle": "2025-09-10T23:12:24.773449Z",
     "shell.execute_reply": "2025-09-10T23:12:24.772650Z",
     "shell.execute_reply.started": "2025-09-10T23:12:23.985960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: He go to school yesterday.\n",
      "Prediction: He went to school yesterday.\n",
      "----------\n",
      "Input 2: I has a pen.\n",
      "Prediction: I have a pen.\n",
      "----------\n",
      "Input 3: He suggested me to go to the doctor because I am sick.\n",
      "Prediction: He suggested I go to the doctor because I am sick.\n",
      "----------\n",
      "Input 4: Despite of being tired, but she continued working.\n",
      "Prediction: Despite being tired, she continued working.\n",
      "----------\n",
      "Input 5: I have visited Paris last year for the first time.\n",
      "Prediction: I visited Paris last year for the first time.\n",
      "----------\n",
      "Input 6: Everyone should knows their responsibilities\n",
      "Prediction: Everyone should know his or her responsibilities.\n",
      "----------\n",
      "Input 7: She don’t knows nothing about the project yet.\n",
      "Prediction: She doesn’t know anything about the project yet.\n",
      "----------\n",
      "Input 8: Him and me was going to the market yesterday.\n",
      "Prediction: He and I were going to the market yesterday.\n",
      "----------\n",
      "Input 9: If I would have seen her, I would tell her the truth.\n",
      "Prediction: If I had seen her, I would have told her the truth.\n",
      "----------\n",
      "Input 10: Running fastly, the race was won by him.\n",
      "Prediction: Running fast, the race was won by him.\n",
      "----------\n",
      "Input 11: The informations you gave me are very helpful.\n",
      "Prediction: The information you gave me are very helpful.\n",
      "----------\n",
      "Input 12: I am agree with you about this idea.\n",
      "Prediction: I agree with you about this idea.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./best_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
    "\n",
    "# جمل تجريبية\n",
    "sentences = [\n",
    "    \"He go to school yesterday.\",\n",
    "    \"I has a pen.\",\n",
    "    \"He suggested me to go to the doctor because I am sick.\",\n",
    "    \"Despite of being tired, but she continued working.\",\n",
    "    \"I have visited Paris last year for the first time.\",\n",
    "    \"Everyone should knows their responsibilities\",\n",
    "    \"She don’t knows nothing about the project yet.\",\n",
    "    \"Him and me was going to the market yesterday.\",\n",
    "    \"If I would have seen her, I would tell her the truth.\",\n",
    "    \"Running fastly, the race was won by him.\",\n",
    "    \"The informations you gave me are very helpful.\",\n",
    "    \"I am agree with you about this idea.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "counter=1\n",
    "for src, pred in zip(sentences, preds):\n",
    "    print(f\"Input {counter}: {src}\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(\"----------\")\n",
    "    counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#he made only oen mistake in input 11"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4190066,
     "sourceId": 7235673,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
